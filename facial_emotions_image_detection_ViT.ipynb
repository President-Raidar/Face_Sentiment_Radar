{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e59e8b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q evaluate==0.4.3 transformers matplotlib scikit-learn imblearn accelerate torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211e9c4",
   "metadata": {},
   "source": [
    "Import libraries, load and transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309e5913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Git/Face_Sentiment_Radar/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.3\n"
     ]
    }
   ],
   "source": [
    "# Install necessary Python packages using pip\n",
    "\n",
    "# Use the 'pip' command to install packages\n",
    "# The '-q' flag stands for 'quiet,' which means it will suppress most output, making the installation process less verbose\n",
    "# We're installing the following packages:\n",
    "# - 'evaluate': This package is likely used for evaluation purposes, but the specific functionality is not clear from this line alone\n",
    "# - 'transformers': This package is commonly used for natural language processing tasks, such as working with pre-trained language models like BERT or GPT\n",
    "# - 'datasets': This package provides easy access to various datasets commonly used in machine learning and natural language processing tasks\n",
    "# - 'mlflow': MLflow is an open-source platform for managing the end-to-end machine learning lifecycle, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models\n",
    "\n",
    "# Note: Before running this code, make sure you have Python and pip installed on your system.\n",
    "# Also, ensure you have an internet connection since pip will download and install these packages from PyPI (Python Package Index).\n",
    "\n",
    "\n",
    "import evaluate\n",
    "print(evaluate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4377afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and modules\n",
    "import warnings  # Import the 'warnings' module for handling warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings during execution\n",
    "\n",
    "import gc  # Import the 'gc' module for garbage collection\n",
    "import numpy as np  # Import NumPy for numerical operations\n",
    "import pandas as pd  # Import Pandas for data manipulation\n",
    "import itertools  # Import 'itertools' for iterators and looping\n",
    "from collections import Counter  # Import 'Counter' for counting elements\n",
    "import matplotlib.pyplot as plt  # Import Matplotlib for data visualization\n",
    "from sklearn.metrics import (  # Import various metrics from scikit-learn\n",
    "    accuracy_score,  # For calculating accuracy\n",
    "    roc_auc_score,  # For ROC AUC score\n",
    "    confusion_matrix,  # For confusion matrix\n",
    "    classification_report,  # For classification report\n",
    "    f1_score  # For F1 score\n",
    ")\n",
    "\n",
    "# Import custom modules and classes\n",
    "from imblearn.over_sampling import RandomOverSampler # import RandomOverSampler\n",
    "import accelerate # Import the 'accelerate' module\n",
    "import evaluate  # Import the 'evaluate' module\n",
    "from datasets import Dataset, Image, ClassLabel  # Import custom 'Dataset', 'ClassLabel', and 'Image' classes\n",
    "from transformers import (  # Import various modules from the Transformers library\n",
    "    TrainingArguments,  # For training arguments\n",
    "    Trainer,  # For model training\n",
    "    ViTImageProcessor,  # For processing image data with ViT models\n",
    "    ViTForImageClassification,  # ViT model for image classification\n",
    "    DefaultDataCollator  # For collating data in the default way\n",
    ")\n",
    "import torch  # Import PyTorch for deep learning\n",
    "from torch.utils.data import DataLoader  # For creating data loaders\n",
    "from torchvision.transforms import (  # Import image transformation functions\n",
    "    CenterCrop,  # Center crop an image\n",
    "    Compose,  # Compose multiple image transformations\n",
    "    Normalize,  # Normalize image pixel values\n",
    "    RandomRotation,  # Apply random rotation to images\n",
    "    RandomResizedCrop,  # Crop and resize images randomly\n",
    "    RandomHorizontalFlip,  # Apply random horizontal flip\n",
    "    RandomAdjustSharpness,  # Adjust sharpness randomly\n",
    "    Resize,  # Resize images\n",
    "    ToTensor  # Convert images to PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd5c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary module from the Python Imaging Library (PIL).\n",
    "from PIL import ImageFile\n",
    "\n",
    "# Enable the option to load truncated images.\n",
    "# This setting allows the PIL library to attempt loading images even if they are corrupted or incomplete.\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "820d2c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35887 35887\n",
      "(35887, 2)\n"
     ]
    }
   ],
   "source": [
    "# use https://huggingface.co/docs/datasets/image_load for reference\n",
    "\n",
    "# Import necessary libraries\n",
    "image_dict = {}\n",
    "\n",
    "# Define the list of file names\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "# Initialize empty lists to store file names and labels\n",
    "file_names = []\n",
    "labels = []\n",
    "\n",
    "# Iterate through all image files in the specified directory\n",
    "for file in sorted((Path('images/images/').glob('*/*/*.*'))):\n",
    "    # check number of such files in a directory\n",
    "    sample_dir = '/'.join(str(file).split('/')[:-1])+'/'\n",
    "    \n",
    "    label = str(file).split('/')[-2]  # Extract the label from the file path\n",
    "    labels.append(label)  # Add the label to the list\n",
    "    file_names.append(str(file))  # Add the file path to the list\n",
    "\n",
    "# Print the total number of file names and labels\n",
    "print(len(file_names), len(labels))\n",
    "\n",
    "# Create a pandas dataframe from the collected file names and labels\n",
    "df = pd.DataFrame.from_dict({\"image\": file_names, \"label\": labels})\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2b313f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>images/images/train/angry/0.jpg</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>images/images/train/angry/1.jpg</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>images/images/train/angry/10.jpg</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>images/images/train/angry/10002.jpg</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>images/images/train/angry/10016.jpg</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 image  label\n",
       "0      images/images/train/angry/0.jpg  angry\n",
       "1      images/images/train/angry/1.jpg  angry\n",
       "2     images/images/train/angry/10.jpg  angry\n",
       "3  images/images/train/angry/10002.jpg  angry\n",
       "4  images/images/train/angry/10016.jpg  angry"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a146a7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62923, 2)\n"
     ]
    }
   ],
   "source": [
    "# random oversampling of minority class\n",
    "# 'y' contains the target variable (label) we want to predict\n",
    "y = df[['label']]\n",
    "\n",
    "# Drop the 'label' column from the DataFrame 'df' to separate features from the target variable\n",
    "df = df.drop(['label'], axis=1)\n",
    "\n",
    "# Create a RandomOverSampler object with a specified random seed (random_state=83)\n",
    "ros = RandomOverSampler(random_state=83)\n",
    "\n",
    "# Use the RandomOverSampler to resample the dataset by oversampling the minority class\n",
    "# 'df' contains the feature data, and 'y_resampled' will contain the resampled target variable\n",
    "df, y_resampled = ros.fit_resample(df, y)\n",
    "\n",
    "# Delete the original 'y' variable to save memory as it's no longer needed\n",
    "del y\n",
    "\n",
    "# Add the resampled target variable 'y_resampled' as a new 'label' column in the DataFrame 'df'\n",
    "df['label'] = y_resampled\n",
    "\n",
    "# Delete the 'y_resampled' variable to save memory as it's no longer needed\n",
    "del y_resampled\n",
    "\n",
    "# Perform garbage collection to free up memory used by discarded variables\n",
    "gc.collect()\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db694197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from a Pandas DataFrame.\n",
    "dataset = Dataset.from_pandas(df).cast_column(\"image\", Image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22725deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAwADABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APP9Ihu9XmRI1XHd3OcfhXpui+HbHT7faEEsjD55HHJ/wFbKaJp0jbmtISR3Kita3srWJNscKAD0Fcb4/s0tkt7mLCCYtCx7bsZU/oRXlM5feQeVxzx3ruvC2jjTdOWQnMjruPtW6mv2dmSspkJ7hRmrVt4s0eSXyhf+W3o69K24NatZMHcCvADg8E1zHxKu0m8P2vlurgXaElWzgBWry6UAqw9q9P8ADxE9lCJRtYoMg9qm1XwjbXqkiNSxB+YsRwfT0NZC+ArWOOMhSnlrjO7Jc5zk1oa14Vubnw9ax6deNblJD5m0Z3DHFcp4n0waPBBaLJIyyIZgH6jkD+hrn3+7nPOK7zTpWgdBnC9K6yzus4DE49O1VNX1a2tZE86dbeHBJduB9K0NJ1rSr20WG2vI5nY5Cq2ciuA+IVyl1rCxpj9xAFOPUkmuRQhoB9MV1NlfI+zn9etdPZ3eYiT1WszU9f0xwLe8eJQrdGQkj9KuaNe6VZWF3c2jQtHGpkdoxjp259c1yKabqetvc6i0LCA7nkncEIPYev0FVdR0SbSZAn2m1u4z0ltZQwz6HuDWd5l1p7ktGzwg5DqMgf4V0+iawtw67ZB9M10sq6w4JtfsjKQCDLz+lZ97dPrOq2nhuaW3geRTJcurbflHIVfcnt7VNrelXNzfWmlWKh7KOPYGZtoVgffrxiuWudLn0m6l3QrKsZIlgI3BvcV//9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=48x48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first image in the dataset\n",
    "dataset[0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2754c7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angry', 'angry', 'angry', 'angry', 'angry']\n"
     ]
    }
   ],
   "source": [
    "# Extracting a subset of elements from the 'labels' list using slicing.\n",
    "# The slicing syntax [:5] selects elements from the beginning up to (but not including) the 5th element.\n",
    "# This will give us the first 5 elements of the 'labels' list.\n",
    "# The result will be a new list containing these elements.\n",
    "labels_subset = labels[:5]\n",
    "\n",
    "# Printing the subset of labels to inspect the content.\n",
    "print(labels_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4300e5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of IDs to Labels: {0: 'sad', 1: 'disgust', 2: 'angry', 3: 'neutral', 4: 'fear', 5: 'surprise', 6: 'happy'} \n",
      "\n",
      "Mapping of Labels to IDs: {'sad': 0, 'disgust': 1, 'angry': 2, 'neutral': 3, 'fear': 4, 'surprise': 5, 'happy': 6}\n"
     ]
    }
   ],
   "source": [
    "# Create a list of unique labels by converting 'labels' to a set and then back to a list\n",
    "labels_list = ['sad', 'disgust', 'angry', 'neutral', 'fear', 'surprise', 'happy'] # list(set(labels))\n",
    "\n",
    "# Initialize empty dictionaries to map labels to IDs and vice versa\n",
    "label2id, id2label = dict(), dict()\n",
    "\n",
    "# Iterate over the unique labels and assign each label an ID, and vice versa\n",
    "for i, label in enumerate(labels_list):\n",
    "    label2id[label] = i  # Map the label to its corresponding ID\n",
    "    id2label[i] = label  # Map the ID to its corresponding label\n",
    "\n",
    "# Print the resulting dictionaries for reference\n",
    "print(\"Mapping of IDs to Labels:\", id2label, '\\n')\n",
    "print(\"Mapping of Labels to IDs:\", label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6d9af43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 62923/62923 [00:00<00:00, 1320336.94 examples/s]\n",
      "Casting the dataset: 100%|██████████| 62923/62923 [00:00<00:00, 16390398.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating classlabels to match labels to IDs\n",
    "ClassLabels = ClassLabel(num_classes=len(labels_list), names=labels_list)\n",
    "\n",
    "# Mapping labels to IDs\n",
    "def map_label2id(example):\n",
    "    example['label'] = ClassLabels.str2int(example['label'])\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(map_label2id, batched=True)\n",
    "\n",
    "# Casting label column to ClassLabel Object\n",
    "dataset = dataset.cast_column('label', ClassLabels)\n",
    "\n",
    "# Splitting the dataset into training and testing sets using an 60-40 split ratio.\n",
    "dataset = dataset.train_test_split(test_size=0.4, shuffle=True, stratify_by_column=\"label\")\n",
    "\n",
    "# Extracting the training data from the split dataset.\n",
    "train_data = dataset['train']\n",
    "\n",
    "# Extracting the testing data from the split dataset.\n",
    "test_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b61337e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:  224\n"
     ]
    }
   ],
   "source": [
    "# Define the pre-trained ViT model string\n",
    "model_str = 'google/vit-base-patch16-224-in21k' #\"dima806/facial_emotions_image_detection\"\n",
    "\n",
    "# Create a processor for ViT model input from the pre-trained model\n",
    "processor = ViTImageProcessor.from_pretrained(model_str)\n",
    "\n",
    "# Retrieve the image mean and standard deviation used for normalization\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "\n",
    "# Get the size (height) of the ViT model's input images\n",
    "size = processor.size[\"height\"]\n",
    "print(\"Size: \", size)\n",
    "\n",
    "# Define a normalization transformation for the input images\n",
    "normalize = Normalize(mean=image_mean, std=image_std)\n",
    "\n",
    "# Define a set of transformations for training data\n",
    "_train_transforms = Compose(\n",
    "    [\n",
    "        Resize((size, size)),             # Resize images to the ViT model's input size\n",
    "        RandomRotation(90),               # Apply random rotation\n",
    "        RandomAdjustSharpness(2),         # Adjust sharpness randomly\n",
    "        RandomHorizontalFlip(0.5),        # Random horizontal flip\n",
    "        ToTensor(),                       # Convert images to tensors\n",
    "        normalize                         # Normalize images using mean and std\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a set of transformations for validation data\n",
    "_val_transforms = Compose(\n",
    "    [\n",
    "        Resize((size, size)),             # Resize images to the ViT model's input size\n",
    "        ToTensor(),                       # Convert images to tensors\n",
    "        normalize                         # Normalize images using mean and std\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a function to apply training transformations to a batch of examples\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "# Define a function to apply validation transformations to a batch of examples\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90789fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the transforms for the training data\n",
    "train_data.set_transform(train_transforms)\n",
    "\n",
    "# Set the transforms for the test/validation data\n",
    "test_data.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "661defe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a collate function that prepares batched data for model training.\n",
    "def collate_fn(examples):\n",
    "    # Stack the pixel values from individual examples into a single tensor.\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    \n",
    "    # Convert the label strings in examples to corresponding numeric IDs using label2id dictionary.\n",
    "    labels = torch.tensor([example['label'] for example in examples])\n",
    "    \n",
    "    # Return a dictionary containing the batched pixel values and labels.\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ef920",
   "metadata": {},
   "source": [
    "# Load, train, and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fb916b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.804039\n"
     ]
    }
   ],
   "source": [
    "# Create a ViTForImageClassification model from a pretrained checkpoint with a specified number of output labels.\n",
    "model = ViTForImageClassification.from_pretrained(model_str, num_labels=len(labels_list))\n",
    "\n",
    "# Configure the mapping of class labels to their corresponding indices for later reference.\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "\n",
    "# Calculate and print the number of trainable parameters in millions for the model.\n",
    "print(model.num_parameters(only_trainable=True) / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e321d4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 19.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the accuracy metric from a module named 'evaluate'\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Define a function 'compute_metrics' to calculate evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    # Extract model predictions from the evaluation prediction object\n",
    "    predictions = eval_pred.predictions\n",
    "    \n",
    "    # Extract true labels from the evaluation prediction object\n",
    "    label_ids = eval_pred.label_ids\n",
    "    \n",
    "    # Calculate accuracy using the loaded accuracy metric\n",
    "    # Convert model predictions to class labels by selecting the class with the highest probability (argmax)\n",
    "    predicted_labels = predictions.argmax(axis=1)\n",
    "    \n",
    "    # Calculate accuracy score by comparing predicted labels to true labels\n",
    "    acc_score = accuracy.compute(predictions=predicted_labels, references=label_ids)['accuracy']\n",
    "    \n",
    "    # Return the computed accuracy as a dictionary with the key \"accuracy\"\n",
    "    return {\n",
    "        \"accuracy\": acc_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb399186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53ec3964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the evaluation metric to be used during training and evaluation.\n",
    "metric_name = \"accuracy\"\n",
    "\n",
    "# Define the name of the model, which will be used to create a directory for saving model checkpoints and outputs.\n",
    "model_name = \"facial_emotions_image_detection\"\n",
    "\n",
    "# Define the number of training epochs for the model.\n",
    "num_train_epochs = 25\n",
    "\n",
    "# Create an instance of TrainingArguments to configure training settings.\n",
    "args = TrainingArguments(\n",
    "    # Specify the directory where model checkpoints and outputs will be saved.\n",
    "    output_dir=model_name,\n",
    "    \n",
    "    # Specify the directory where training logs will be stored.\n",
    "    logging_dir='./logs',\n",
    "    \n",
    "    # Set the learning rate for the optimizer.\n",
    "    learning_rate=3e-6,\n",
    "    \n",
    "    # Define the batch size for training on each device.\n",
    "    per_device_train_batch_size=32,\n",
    "    \n",
    "    # Define the batch size for evaluation on each device.\n",
    "    per_device_eval_batch_size=8,\n",
    "    \n",
    "    # Specify the total number of training epochs.\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    \n",
    "    # Apply weight decay to prevent overfitting.\n",
    "    weight_decay=0.02,\n",
    "    \n",
    "    # Set the number of warm-up steps for the learning rate scheduler.\n",
    "    warmup_steps=50,\n",
    "    \n",
    "    # Disable the removal of unused columns from the dataset.\n",
    "    remove_unused_columns=False,\n",
    "    \n",
    "    # Load the best model at the end of training.\n",
    "    # load_best_model_at_end=True,\n",
    "    \n",
    "    # Set evaluation and save strategies to steps for compatibility.\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # save_strategy=\"steps\",\n",
    "    # Instead of evaluation_strategy=\"steps\", use:\n",
    "    eval_steps=100,  # Evaluate every 100 steps\n",
    "    \n",
    "    # Instead of save_strategy=\"steps\", use:\n",
    "    save_steps=100,  # Save every 100 steps\n",
    "    \n",
    "    # Limit the total number of saved checkpoints to save space.\n",
    "    # save_total_limit=1,\n",
    "    \n",
    "    # Specify that training progress should not be reported.\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fcd8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Trainer instance for fine-tuning a language model.\n",
    "\n",
    "# - `model`: The pre-trained language model to be fine-tuned.\n",
    "# - `args`: Configuration settings and hyperparameters for training.\n",
    "# - `train_dataset`: The dataset used for training the model.\n",
    "# - `eval_dataset`: The dataset used for evaluating the model during training.\n",
    "# - `data_collator`: A function that defines how data batches are collated and processed.\n",
    "# - `compute_metrics`: A function for computing custom evaluation metrics.\n",
    "# - `tokenizer`: The tokenizer used for processing text data.\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05f4b944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3147' max='3147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3147/3147 10:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.9446556568145752,\n",
       " 'eval_model_preparation_time': 0.0028,\n",
       " 'eval_accuracy': 0.1590782677791021,\n",
       " 'eval_runtime': 636.0768,\n",
       " 'eval_samples_per_second': 39.571,\n",
       " 'eval_steps_per_second': 4.948}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the pre-training model's performance on a test dataset.\n",
    "# This function calculates various metrics such as accuracy, loss, etc.,\n",
    "# to assess how well the model is performing on unseen data.\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f3e5b96",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Start training the model using the trainer object.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Face_Sentiment_Radar/.venv/lib/python3.13/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Face_Sentiment_Radar/.venv/lib/python3.13/site-packages/transformers/trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Face_Sentiment_Radar/.venv/lib/python3.13/site-packages/transformers/trainer.py:3782\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3779\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3780\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3782\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3784\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Face_Sentiment_Radar/.venv/lib/python3.13/site-packages/accelerate/accelerator.py:2454\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2452\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2453\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2454\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Face_Sentiment_Radar/.venv/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Face_Sentiment_Radar/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Face_Sentiment_Radar/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Start training the model using the trainer object.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775590bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the post-training model's performance on the validation or test dataset.\n",
    "# This function computes various evaluation metrics like accuracy, loss, etc.\n",
    "# and provides insights into how well the model is performing.\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26587be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained 'trainer' to make predictions on the 'test_data'.\n",
    "outputs = trainer.predict(test_data)\n",
    "\n",
    "# Print the metrics obtained from the prediction outputs.\n",
    "print(outputs.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95defdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the true labels from the model outputs\n",
    "y_true = outputs.label_ids\n",
    "\n",
    "# Predict the labels by selecting the class with the highest probability\n",
    "y_pred = outputs.predictions.argmax(1)\n",
    "\n",
    "# Define a function to plot a confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    This function plots a confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "        cm (array-like): Confusion matrix as returned by sklearn.metrics.confusion_matrix.\n",
    "        classes (list): List of class names, e.g., ['Class 0', 'Class 1'].\n",
    "        title (str): Title for the plot.\n",
    "        cmap (matplotlib colormap): Colormap for the plot.\n",
    "    \"\"\"\n",
    "    # Create a figure with a specified size\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Display the confusion matrix as an image with a colormap\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Define tick marks and labels for the classes on the axes\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.0f'\n",
    "    # Add text annotations to the plot indicating the values in the cells\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    # Label the axes\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    # Ensure the plot layout is tight\n",
    "    plt.tight_layout()\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# Display accuracy and F1 score\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Get the confusion matrix if there are a small number of labels\n",
    "if len(labels_list) <= 150:\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Plot the confusion matrix using the defined function\n",
    "    plot_confusion_matrix(cm, labels_list, figsize=(8, 6))\n",
    "    \n",
    "# Finally, display classification report\n",
    "print()\n",
    "print(\"Classification report:\")\n",
    "print()\n",
    "print(classification_report(y_true, y_pred, target_names=labels_list, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d0f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model: This line of code is responsible for saving the model\n",
    "# that has been trained using the trainer object. It will serialize the model\n",
    "# and its associated weights, making it possible to reload and use the model\n",
    "# in the future without the need to retrain it.\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9821c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'pipeline' function from the 'transformers' library.\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline for image classification tasks. \n",
    "# You need to specify the 'model_name' and the 'device' to use for inference.\n",
    "# - 'model_name': The name of the pre-trained model to be used for image classification.\n",
    "# - 'device': Specifies the device to use for running the model (0 for GPU, -1 for CPU).\n",
    "pipe = pipeline('image-classification', model=model_name, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e018be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing an image from the 'test_data' dataset using index 1.\n",
    "image = test_data[1][\"image\"]\n",
    "\n",
    "# Displaying the 'image' variable.\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the 'pipe' function to process the 'image' variable.\n",
    "pipe(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8166296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line of code accesses the \"label\" attribute of a specific element in the test_data list.\n",
    "# It's used to retrieve the actual label associated with a test data point.\n",
    "id2label[test_data[1][\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a873d6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Git/Face_Sentiment_Radar/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Emotions: [{'label': 'neutral', 'score': 0.7695603966712952}, {'label': 'happy', 'score': 0.09537415951490402}, {'label': 'sad', 'score': 0.04011106118559837}, {'label': 'surprise', 'score': 0.0378488190472126}, {'label': 'angry', 'score': 0.03305353969335556}]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Load the pre-trained model from Hugging Face\n",
    "model_name = 'dima806/facial_emotions_image_detection'\n",
    "emotion_analyzer = pipeline('image-classification', model=model_name, device=0)\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = 'test_trump.jpeg'  # Replace with your image path\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Analyze the emotion in the image\n",
    "results = emotion_analyzer(image)\n",
    "\n",
    "# Display the results\n",
    "print('Predicted Emotions:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ca058aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in ./.venv/lib/python3.13/site-packages (from opencv-python) (2.2.5)\n",
      "Using cached opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl (37.3 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0edb78ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Emotions: [{'label': 'happy', 'score': 0.6967912316322327}, {'label': 'neutral', 'score': 0.16989006102085114}, {'label': 'sad', 'score': 0.036068160086870193}, {'label': 'fear', 'score': 0.03589561581611633}, {'label': 'angry', 'score': 0.0229779202491045}]\n"
     ]
    }
   ],
   "source": [
    "# Import OpenCV for face detection\n",
    "import cv2\n",
    "\n",
    "# Define a function to detect and crop faces from an image\n",
    "def detect_and_crop_face(image_path):\n",
    "    # Load the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale for face detection\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Load the pre-trained Haar Cascade classifier for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Detect faces in the image\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    # If no faces are detected, return the original image\n",
    "    if len(faces) == 0:\n",
    "        print(\"No faces detected.\")\n",
    "        return image\n",
    "    \n",
    "    # Crop the first detected face (assuming one face per image)\n",
    "    x, y, w, h = faces[0]\n",
    "    cropped_face = image[y:y+h, x:x+w]\n",
    "    \n",
    "    # Return the cropped face\n",
    "    return cropped_face\n",
    "\n",
    "# Update the image preprocessing step to include face detection and cropping\n",
    "image_path = 'test_trump.jpeg'  # Replace with your image path\n",
    "cropped_image = detect_and_crop_face(image_path)\n",
    "\n",
    "# Convert the cropped image to PIL format for compatibility with the pipeline\n",
    "from PIL import Image\n",
    "cropped_image = Image.fromarray(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Analyze the emotion in the cropped image\n",
    "results = emotion_analyzer(cropped_image)\n",
    "\n",
    "# Display the results\n",
    "print('Predicted Emotions:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0af1fba9",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Update the image processing and display step\u001b[39;00m\n\u001b[32m     43\u001b[39m image_path = \u001b[33m'\u001b[39m\u001b[33mtest_trump2.jpg\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# Replace with your image path\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m cropped_image = \u001b[43mdetect_and_crop_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Convert the cropped image to PIL format for compatibility with the pipeline\u001b[39;00m\n\u001b[32m     47\u001b[39m cropped_image = Image.fromarray(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mdetect_and_crop_face\u001b[39m\u001b[34m(image_path)\u001b[39m\n\u001b[32m      7\u001b[39m image = cv2.imread(image_path)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Convert the image to grayscale for face detection\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m gray = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Load the pre-trained Haar Cascade classifier for face detection\u001b[39;00m\n\u001b[32m     13\u001b[39m face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \u001b[33m'\u001b[39m\u001b[33mhaarcascade_frontalface_default.xml\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "# Modify the function to display results on the image\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def detect_and_display_face_with_emotion(image_path, results):\n",
    "    # Load the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Image file '{image_path}' not found.\")\n",
    "    \n",
    "    # Convert the image to grayscale for face detection\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Load the pre-trained Haar Cascade classifier for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Detect faces in the image\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    # If no faces are detected, return the original image\n",
    "    if len(faces) == 0:\n",
    "        print(\"No faces detected.\")\n",
    "        return image\n",
    "\n",
    "    # Draw rectangles around detected faces and add emotion text\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Get the top emotion result\n",
    "        top_emotion = results[0]\n",
    "        emotion_text = f\"{top_emotion['label']} ({top_emotion['score'] * 100:.2f}%)\"\n",
    "        \n",
    "        # Add the emotion text next to the face\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(image, emotion_text, (x, y - 10), font, 0.5, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Convert the image back to RGB for displaying with PIL\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(image_rgb)\n",
    "\n",
    "# Update the image processing and display step\n",
    "image_path = 'test_trump2.jpg'  # Replace with your image path\n",
    "cropped_image = detect_and_crop_face(image_path)\n",
    "\n",
    "# Convert the cropped image to PIL format for compatibility with the pipeline\n",
    "cropped_image = Image.fromarray(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Analyze the emotion in the cropped image\n",
    "results = emotion_analyzer(cropped_image)\n",
    "\n",
    "# Display the results on the original image\n",
    "result_image = detect_and_display_face_with_emotion(image_path, results)\n",
    "\n",
    "# Show the image with annotations\n",
    "result_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
